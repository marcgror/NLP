{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37764bitmyenvconda966a52bc464c4d8495af3e092d2af30f",
   "display_name": "Python 3.7.7 64-bit ('myenv': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data, from tensorflow datasets, and show the different sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "dict_keys(['test', 'train', 'unsupervised'])"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "datasets, info = tfds.load('imdb_reviews', as_supervised=True, with_info=True)\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the different datasets's size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = info.splits['train'].num_examples\n",
    "test_size = info.splits['test'].num_examples\n",
    "unsupervised_size = info.splits['unsupervised'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(25000, 25000, 50000)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train_size, test_size, unsupervised_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the IMDB dataset is composet of 25000 reviews for training, 25000 for testing and 50000 without labels, that we can use for predict, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to preprocess the data, let's see two review with their respective assigned labels for the train, test and unsupervised datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Review: This is a big step down after the surprisingly enjoyable original. This sequel isn't nearly as fun as part one, and it instead spends too much time on plot development. Tim Thomerson is still the best ...\nLabel: 0 = Negative\n\nReview: Perhaps because I was so young, innocent and BRAINWASHED when I saw it, this movie was the cause of many sleepless nights for me. I haven't seen it since I was in seventh grade at a Presbyterian schoo ...\nLabel: 0 = Negative\n\n"
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets['train'].batch(2).take(1):\n",
    "        for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "            print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "            print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Review: It opens with your cliche overly long ship flying through space. All I could think at this point was \"Spaceballs\" and hoping there'd be a sticker on back that said \"We break for Nobody.\" The movie the ...\nLabel: 1 = Positive\n\nReview: I remember seeing this at my local Blockbuster and picked it up cause I was curious. I liked movies about mythological creatures. I like movies about werewolves, vampires, zombies, etc. This is based  ...\nLabel: 0 = Negative\n\n"
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets['test'].batch(2).take(1):\n",
    "        for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "            print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "            print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Review: I'm baffled by the number of people who actually liked this movie. How anyone can watch this crud is beyond me. Aside from the blatant attempt to cash in on the popularity of Arthurian legend (this st ...\nLabel: -1 = Positive\n\nReview: Well this is a real mess of a film. It's worthy of watching at a drive-in, where you don't have to pay much attention to the plot because you are too busy ... doing other things. Watch it on DVD, howe ...\nLabel: -1 = Positive\n\n"
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets['unsupervised'].batch(2).take(1):\n",
    "        for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "            print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "            print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we define an initial function useful for preprocessing the text reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess (X_batch, y_batch):\n",
    "    # truncate the reviews to the first 300 characters\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    # replace <br /> and any other characters other than letters and quotes with spaces\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"<br\\\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    # split the review by the spaces\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    # returns a dense tensor, with all the reviews padded so they all have the same length\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we store the preprocessing proces in a function, that can be applied later in any dataset(train, test, unsupervised):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "num_oov_buckets = 1000\n",
    "def function (datasets, set):\n",
    "    # initialize a Counter object\n",
    "    vocabulary = Counter()\n",
    "    # go through the whole dataset, apply preprocess function and count word ocurrencies\n",
    "    for X_batch, y_batch in datasets[set].batch(32).map(preprocess):\n",
    "        for review in X_batch:\n",
    "            vocabulary.update(list(review.numpy()))\n",
    "    # truncate our vocabulary to only a number of words\n",
    "    truncated_vocabulary = [\n",
    "        word for word, count in vocabulary.most_common()[:vocab_size]]\n",
    "    # replace each word with its ID (index in the vocabulary)\n",
    "    words = tf.constant(truncated_vocabulary)\n",
    "    words_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "    vocab_init = tf.lookup.KeyValueTensorInitializer(words, words_ids)\n",
    "    table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)\n",
    "    # encode the words\n",
    "    def encode_words(X_batch, y_batch):\n",
    "        return table.lookup(X_batch), y_batch\n",
    "    # batch, preprocess, encode and prefetch the reviews\n",
    "    data_set = datasets[set].repeat().batch(32).map(preprocess)\n",
    "    data_set = data_set.map(encode_words).prefetch(1)\n",
    "    # return the prepared dataset\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply this whole preprocessing process to our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = function(datasets, 'train')\n",
    "test_set = function(datasets, 'test')\n",
    "unsupervised_set = function(datasets, 'unsupervised')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At least we can create the model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train for 781 steps\nEpoch 1/5\n781/781 [==============================] - 41s 52ms/step - loss: 0.5882 - accuracy: 0.6592\nEpoch 2/5\n781/781 [==============================] - 34s 44ms/step - loss: 0.3685 - accuracy: 0.8443\nEpoch 3/5\n781/781 [==============================] - 35s 45ms/step - loss: 0.2235 - accuracy: 0.9165\nEpoch 4/5\n781/781 [==============================] - 34s 44ms/step - loss: 0.1507 - accuracy: 0.9457\nEpoch 5/5\n781/781 [==============================] - 36s 46ms/step - loss: 0.1262 - accuracy: 0.9563\n"
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    # convert word IDs into embeddings\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, input_shape=[None]),\n",
    "    # two GRU layers\n",
    "    keras.layers.GRU(128, dropout=0.2, return_sequences=True),\n",
    "    keras.layers.GRU(128, dropout=0.2, return_sequences=True),\n",
    "    keras.layers.GRU(128, dropout=0.2),\n",
    "    # dense layer with sigmoid activation to output the estimated probabilities\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can evaluate it with the test set to look for overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "781/781 [==============================] - 11s 15ms/step - loss: 2.7816 - accuracy: 0.4996\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[2.781618276265153, 0.49955985]"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "model.evaluate(test_set, steps=test_size // 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final exercise, we can make predictions from the unsupervised dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(unsupervised_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display a few reviews with their predicted label, to check the generalization of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Review: I'm baffled by the number of people who actually liked this movie. How anyone can watch this crud is beyond me. Aside from the blatant attempt to cash in on the popularity of Arthurian legend (this st ...\nLabel: [0.33107972]\n------------------------------\nReview: Well this is a real mess of a film. It's worthy of watching at a drive-in, where you don't have to pay much attention to the plot because you are too busy ... doing other things. Watch it on DVD, howe ...\nLabel: [0.7999851]\n------------------------------\nReview: During the past six months or so, I have intentionally sat and watched hordes of terrible movies, in search of the most entertainingly godawful ones I can. I've probably been through about 50 or so. R ...\nLabel: [0.65163875]\n------------------------------\nReview: A story about a grown-up pair of Siamese twin brothers - one does wonder. \"Twin Falls Idaho\" is a quiet yet possibly disturbing film (only if one is uncomfortable with the idea of looking at a pair of ...\nLabel: [0.6015207]\n------------------------------\nReview: Polish Vampire in Burbank is the story of a reluctant vampire Dupah (Mark Pirro) who has never sucked blood from a victim before. Up until now his father and sister go out on the hunt and bring him ba ...\nLabel: [0.63643885]\n------------------------------\n"
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets['unsupervised'].batch(5).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), predictions[:5]):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "        print ('Label:', label)\n",
    "        print('------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}